# SignSpeak Smart Glove üß§

<div align="center">

![SignSpeak Logo](https://img.shields.io/badge/SignSpeak-Smart%20Glove-blue?style=for-the-badge)

**A wearable assistive device that translates sign language gestures into natural spoken sentences in real-time.**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![ESP32](https://img.shields.io/badge/ESP32-Arduino-green.svg)](https://www.espressif.com/)

</div>

---

## üìã Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Hardware Requirements](#hardware-requirements)
- [Software Requirements](#software-requirements)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Documentation](#documentation)
- [Contributing](#contributing)
- [License](#license)

## üéØ Overview

SignSpeak Smart Glove is an affordable, standalone, privacy-first wearable device designed to bridge the communication gap between the Deaf/Hard-of-Hearing community and the general population. Unlike camera-based solutions, SignSpeak uses embedded sensors to capture gestures and translates them into natural, context-aware spoken sentences in real-time.

### Problem Statement

Over **72 million people worldwide** rely on sign language, yet most of the population cannot understand it. Current camera-based solutions are restrictive, requiring specific lighting and direct lines of sight while posing privacy risks.

### Our Solution

- ‚úÖ **Privacy-First**: No cameras or video recording
- ‚úÖ **Real-Time**: <500ms latency from gesture to speech
- ‚úÖ **Affordable**: ~$80-120 total hardware cost
- ‚úÖ **Portable**: Battery-powered, wireless operation
- ‚úÖ **Context-Aware**: AI-powered natural sentence generation

## ‚ú® Features

- **Real-time Gesture Recognition**: 20Hz sampling rate with >85% accuracy
- **Contextual Intelligence**: Google Gemini integration for natural sentence generation
- **Text-to-Speech**: Multiple TTS backends (offline and online)
- **Wireless Communication**: Bluetooth connectivity up to 10 meters
- **Battery Powered**: 6-8 hours continuous operation
- **Privacy-Focused**: No cameras, all processing on-device or local PC

## üîß Hardware Requirements

- ESP32 Development Board
- 5x Flex Sensors (analog)
- MPU6050 IMU (6-axis accelerometer/gyroscope)
- 1200mAh Li-Ion Battery
- Battery Charging Module
- 5x 10kŒ© Resistors (pull-down for flex sensors)
- Jumper wires and breadboard/PCB
- Fabric glove for mounting

**Total Cost**: ~$80-120 USD

See [hardware/components_list.md](hardware/components_list.md) for detailed component list.

## üíª Software Requirements

- **Python 3.8+**
- **Arduino IDE** with ESP32 board support
- **Google AI Studio API Key** (for Gemini - optional)
- **Internet Connection** (for Gemini and online TTS - optional)

## üöÄ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/SignSpeak-Smart-Glove.git
cd SignSpeak-Smart-Glove
```

### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure API Keys (Optional)

For Gemini contextual processing:

```bash
# Windows
set GEMINI_API_KEY=your_api_key_here

# Linux/Mac
export GEMINI_API_KEY=your_api_key_here
```

Or create a `.env` file:
```
GEMINI_API_KEY=your_api_key_here
```

### 4. Upload Firmware to ESP32

1. Install [Arduino IDE](https://www.arduino.cc/en/software)
2. Add ESP32 board support (see [hardware/libraries.txt](hardware/libraries.txt))
3. Open `hardware/esp32_firmware/esp32.ino`
4. Select board: **ESP32 Dev Module**
5. Upload to your ESP32

## üìñ Usage

### Training the Model

1. **Collect Training Data**:
   ```bash
   python ml/training/data_logger.py
   ```
   Follow prompts to collect gesture samples.

2. **Train the Model**:
   ```bash
   python ml/training/train_model.py
   ```
   This will generate `ml/models/gesture_model.pkl` and `ml/models/label_encoder.pkl`

### Running the Application

**Full Pipeline** (with Gemini and TTS):
```bash
python software/main.py
```

**Simple Inference** (gesture recognition only):
```bash
python ml/inference.py
```

### Configuration

Edit `software/main.py` to configure:
- Serial port (default: COM10)
- Confidence threshold
- Cooldown period
- Enable/disable Gemini and TTS

## üìÅ Project Structure

```
SignSpeak-Smart-Glove/
‚îÇ
‚îú‚îÄ‚îÄ README.md                 ‚≠ê (MOST IMPORTANT)
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ .gitignore
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ problem_statement.md
‚îÇ   ‚îú‚îÄ‚îÄ system_architecture.png
‚îÇ   ‚îú‚îÄ‚îÄ flow_diagram.png
‚îÇ   ‚îú‚îÄ‚îÄ circuit_diagram.png
‚îÇ   ‚îî‚îÄ‚îÄ presentation.pdf
‚îÇ
‚îú‚îÄ‚îÄ hardware/
‚îÇ   ‚îú‚îÄ‚îÄ esp32_firmware/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ esp32.ino
‚îÇ   ‚îú‚îÄ‚îÄ circuit_diagram.fzz
‚îÇ   ‚îî‚îÄ‚îÄ components_list.md
‚îÇ
‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *_dynamic.csv
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_model.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_logger.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gesture_model.pkl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ label_encoder.pkl
‚îÇ   ‚îî‚îÄ‚îÄ inference.py
‚îÇ
‚îú‚îÄ‚îÄ software/
‚îÇ   ‚îú‚îÄ‚îÄ gesture_classifier.py
‚îÇ   ‚îú‚îÄ‚îÄ gemini_language_engine.py
‚îÇ   ‚îú‚îÄ‚îÄ text_to_speech.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ
‚îú‚îÄ‚îÄ demo/
‚îÇ   ‚îú‚îÄ‚îÄ demo_video_link.txt
‚îÇ   ‚îî‚îÄ‚îÄ screenshots/
‚îÇ
‚îî‚îÄ‚îÄ requirements.txt
```

## üìö Documentation

- [Problem Statement](docs/problem_statement.md)
- [Hardware Components List](hardware/components_list.md)
- [System Architecture](docs/system_architecture.png) *(coming soon)*
- [Circuit Diagram](docs/circuit_diagram.png) *(coming soon)*

## üéØ Performance Metrics

- **Accuracy**: >85% gesture classification
- **Latency**: <500ms end-to-end
- **Battery Life**: 6-8 hours continuous operation
- **Range**: Up to 10 meters Bluetooth range
- **Sampling Rate**: 20Hz

## üîÆ Future Roadmap

- [ ] On-device AI with TensorFlow Lite for Microcontrollers
- [ ] Bilateral communication with OLED display
- [ ] Mobile app (Flutter) for smartphone-based translation
- [ ] Support for more sign language alphabets and phrases
- [ ] Multi-language TTS support

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- ESP32 community for excellent documentation
- Google AI for Gemini API
- Scikit-learn for machine learning tools
- The Deaf/Hard-of-Hearing community for inspiration

## üìß Contact

For questions or support, please open an issue on GitHub.

---

<div align="center">

**Made with ‚ù§Ô∏è for the Deaf/Hard-of-Hearing community**

‚≠ê Star this repo if you find it helpful!

</div>
